{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Processiong de données et RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1. Définiton d'un SparkContext en local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sc = SparkContext => pour RDD utilisé pour données non-structurées (par ex texte)\n",
    "- spark = SparkSession etc... => pour DataFrame utilisé pour données structurées (par ex models de\n",
    "ML).\n",
    "\n",
    "       DF utilise de façon sous-jacente les bases d'un RDD (= SparkContext + SparkSession en sur-couche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de SparkContext du module pyspark\n",
    "from pyspark import SparkContext\n",
    "# Définiton d'un SparkContext en local\n",
    "sc = SparkContext('local')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2. Chargement du fichier \"miserables_full.txt\" et affichage des 10 premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier \"miserables_full.txt\" et affichage des 10 premières lignes\n",
    "miserables = sc.textFile(\"miserables_full.txt\")\n",
    "miserables.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3. succession de méthodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une liste à partir du fichier texte\n",
    "mots_sorted_3 = sc.textFile(\"miserables_full.txt\") \\\n",
    "                  .map(lambda x : x.lower().replace(',', ' ').replace('.', ' ').replace('-', ' ').replace('’', ' ')) \\\n",
    "                  .flatMap(lambda line: line.split(\" \")) \\\n",
    "                  .map(lambda x : (x,1)) \\\n",
    "                  .reduceByKey(lambda x,y : x + y) \\\n",
    "                  .sortBy(lambda couple: couple[1], ascending = False) \\\n",
    "                  .collect()\n",
    "                \n",
    "mots_sorted_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de Spark Session et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext\n",
    "SparkContext.getOrCreate() \n",
    "\n",
    "# Définition d'une SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Introduction au DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un raccourci vers le SparkContext déjà créé\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un DataFrame Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La structure RDD n'est pas optimisée pour effectuer des tâches par colonne ou du Machine Learning. La structure DataFrame a été créé pour répondre à ce besoin. Elle utilise de façon sous-jacente les bases d'un RDD mais a été structurée en colonnes autant qu'en lignes dans une structure SQL et une forme inspirée des DataFrame du module pandas.\n",
    "\n",
    "La structure DataFrame possède deux grands avantages. Tout d'abord cette structure est similaire au DataFrame pandas et est donc facile à prendre en main. Elle est également performante : un DataFrame en PySpark est aussi rapide qu'un DataFrame en Scala et est la structure distribuée la plus optimisée en Machine Learning. Grâce à la structure DataFrame, nous pouvons donc faire des calculs performants à travers un langage familier, en évitant le coût d'entrée d'apprentissage d'un nouveau langage fonctionnel : Scala.\n",
    "\n",
    "Dans cet exercice, vous apprendrez à manipuler un DataFrame PySpark pour explorer les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1. à partir d'un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de Row du package pyspark.sql\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Chargement du fichier '2008_raw.csv'\n",
    "rdd = sc.textFile('2008_raw.csv').map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Création d'un nouveau RDD en sélectionnant les variables explicatives\n",
    "rdd_row = rdd.map(lambda line: Row(annee = line[0],\n",
    "                                   mois = line[1],\n",
    "                                   jours = line[2],\n",
    "                                   flightNum = line[5]))\n",
    "\n",
    "# Créer d'un data frame à partir d'un rdd\n",
    "df = spark.createDataFrame(rdd_row)\n",
    "\n",
    "# Affichage des 5 premières lignes\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2. à partir d'un fichier CSV (méthode plus commune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier '2008.csv'\n",
    "raw_df = spark.read.csv('2008.csv', header=True)\n",
    "\n",
    "# Affichage du schéma des variables\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3. Explorer et manipuler un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un data frame ne contenant que les variables explicatives\n",
    "flights1 = raw_df.select('annee', 'mois', 'jours', 'flightNum', 'origin', 'dest', 'distance', 'canceled', 'cancellationCode', 'carrierDelay')\n",
    "\n",
    "# Affichage de 20 premières lignes\n",
    "flights1.show() # 'show' affiche 20 lignes par défaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame en spécifiant le type des colonnes\n",
    "flights = raw_df.select(raw_df.annee.cast(\"int\"),\n",
    "                        raw_df.mois.cast(\"int\"),\n",
    "                        raw_df.jours.cast(\"int\"),\n",
    "                        raw_df.flightNum.cast(\"int\"),\n",
    "                        raw_df.origin.cast(\"string\"),\n",
    "                        raw_df.dest.cast(\"string\"),\n",
    "                        raw_df.distance.cast(\"int\"),\n",
    "                        raw_df.canceled.cast(\"boolean\"),\n",
    "                        raw_df.cancellationCode.cast(\"string\"),\n",
    "                        raw_df.carrierDelay.cast(\"int\"))\n",
    "\n",
    "# Affichage de 20 premières lignes\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##select disctinct et count\n",
    "# Calcul du nombre de vols ayant des numéros de vol distincts\n",
    "flights.select('flightNum').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##methode describe (2 méthodes)\n",
    "\n",
    "### Première méthode\n",
    "# Affichage d'un résumé en utilisant l'option truncate de la méthode show\n",
    "flights.describe().show(truncate = 8)\n",
    "\n",
    "### Deuxième méthode\n",
    "# Affichage d'un résumé en utilisant la méthode toPandas\n",
    "flights.describe().toPandas()\n",
    "\n",
    "###La méthode toPandas est à utiliser avec des DataFrames de petite taille \n",
    "####tels que des résumés d'informations, sinon elle peut affecter la distribution des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##groupBy (1)\n",
    "# Affichage du résumé de la variable catégorielle 'cancellationCode'\n",
    "flights.groupBy('cancellationCode').count().show()\n",
    "\n",
    "##groupBy (2)\n",
    "# Affichage du résumé de la variable catégorielle 'cancellationCode' et 'canceled'\n",
    "flights.groupBy('cancellationCode', 'canceled').count().show()\n",
    "\n",
    "##filter (1)\n",
    "# Affichage des 20 premièrs vols annulés pour la raison \"C\"\n",
    "flights.filter(flights.cancellationCode == 'C').show()\n",
    "\n",
    "##filter (2)\n",
    "# Calcul du nombre vols annulés par mois\n",
    "flights.filter(flights.canceled == True).groupBy('mois').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.4. Création et aggrégation de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une nouvelle variable 'isLongFlight' et affichage des 10 premières lignes\n",
    "flights.withColumn('isLongFlight', flights.distance > 1000 ).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'enregistrement de la nouvelle colonne ne s'effectue nulle part. A cause du caractère immuable, aucune modification ne se fait par remplacement (in place). Pour enregistrer une nouvelle variable, il faut créer un nouvel objet ou de la créer dès la création du DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.5. Gestion des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fillna\n",
    "# Remplacement des valeurs manquantes par des 0 et affichage des 6 premières lignes\n",
    "flights = flights.fillna(0, 'carrierDelay')\n",
    "flights.show(6)\n",
    "\n",
    "###replace\n",
    "# Remplacement des codes d'annulation : df.replace(oldValue, newValue, 'columnName')\n",
    "flights = flights.replace(['A','B','C'],['1','2','3'],'cancellationCode')\n",
    "flights.show()\n",
    "\n",
    "###orderby\n",
    "# Ordonner le data frame par numéro de vol décroissant\n",
    "flights = flights.orderBy(flights.flightNum.desc())\n",
    "flights.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.6. Requêtes SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) Créer une vue SQL de flights que l'on appellera \"flightsView\".\n",
    "- (b) Créer un DataFrame appelé sqlDF contenant uniquement la variable carrierDelay grâce à une requête SQL.\n",
    "- (c) Afficher les premières lignes de sqlDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une vue SQL\n",
    "flights.createOrReplaceTempView(\"flightsView\")\n",
    "\n",
    "# Création d'un DataFrame ne contenant que la variable \"flightsView\"\n",
    "sqlDF = spark.sql(\"SELECT carrierDelay FROM flightsView\")\n",
    "\n",
    "# Affichage des 10 premières lignes\n",
    "sqlDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.7. Sample & astuces d'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'un dizaine de lignes de la base de données\n",
    "flights.sample(False, .0001, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermeture de la session Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Régression avec PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de SparkSession et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext en local\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction à Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.1. Importation de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) Charger le fichier YearPredictionMSD.txt dans un DataFrame nommé df_raw.\n",
    "- (b) Afficher un extrait de la base de données avec une méthode de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier \" YearPredictionMSD.txt\" dans un DataFrame\n",
    "df_raw = spark.read.csv('YearPredictionMSD.txt')\n",
    "\n",
    "# Première méthode d'affichage \n",
    "df_raw.show(2, truncate = 4)\n",
    "# Modifier les valeurs de 'truncate' ne permet pas de bien visualiser les données\n",
    "# à cause du nombre de variables\n",
    "\n",
    "# Deuxième méthode d'affichage\n",
    "df_raw.sample(False, .00001, seed = 222).toPandas()\n",
    "# Utiliser toPandas permet de mieux visualiser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (c) Importer la fonction col du sous-module pyspark.sql.functions.\n",
    "- (d) Créer un DataFrame df à partir de df_raw en changeant les types des colonnes relatives au timbre en double et l'année en int.\n",
    "- (e) Afficher le schéma des variables du df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de col du sous-module pyspark.sql.functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convertir des colonnes relatives au timbre en double et l'année en int\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns[1:91]]\n",
    "df = df_raw.select(df_raw._c0.cast('int'), *exprs)\n",
    "\n",
    "# Affichage du schéma des variables \"df\"\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Afficher un résumé descriptif de la base de données df.###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'un résumé descriptif des données\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2. Mise en forme de la base en format svmlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir être utilisée par les algorithmes de Machine Learning de Spark ML, la base de données doit être un DataFrame contenant 2 colonnes :\n",
    "\n",
    "    La colonne label contenant la variable à prédire (label en anglais).\n",
    "    La colonne features contenant les variables explicatives (features en anglais).\n",
    "La fonction DenseVector() issue du package pyspark.ml.linalg permet de regrouper plusieurs variables en une seule variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) Importer la fonction DenseVector du package pyspark.ml.linalg.\n",
    "- (b) Créer un rdd rdd_ml séparant la variable à expliquer des features (à mettre sous forme DenseVector).\n",
    "- (c) Créer un DataFrame df_ml contenant notre base de données sous deux variables : 'labels' et 'features'.\n",
    "- (d) Afficher un extrait de df_ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de DenseVector du package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Création d'un rdd en séparant la variable à expliquer des features\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Création d'un DataFrame composé de deux variables : label et features\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "# Affichage des 10 premières lignes du DataFrame\n",
    "df_ml.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décomposition des données en deux ensembles d'entraînement et de test\n",
    "# Par défaut l'échantillon est aléatoirement réparti\n",
    "train, test = df_ml.randomSplit([.8, .2], seed= 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.3. Regression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) Importer la fonction LinearRegression du sous-module pyspark.ml.regression.\n",
    "- (b) Créer lr, une fonction de régression linéaire distribuée pour l'appliquer à l'ensemble train.\n",
    "- (c) Créer linearModel, le modèle issu de lr appliqué à train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de LinearRegression du package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Création d'une fonction de régression linéaire\n",
    "lr = LinearRegression(labelCol='label', featuresCol= 'features')\n",
    "\n",
    "# Apprentissage sur les données d'entraînement \n",
    "linearModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des prédictions sur les données test\n",
    "predicted = linearModel.transform(test)\n",
    "\n",
    "# Affichage des prédictions\n",
    "predicted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.4. Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et affichage du RMSE\n",
    "print(\"RMSE:\", linearModel.summary.rootMeanSquaredError)\n",
    "\n",
    "# Calcul et affichage du R2\n",
    "print(\"R2:  \", linearModel.summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Afficher les coefficients coefficients du modèle. La fonction pprint du module pprint permet d'avoir un affichage plus élégant des données.\n",
    "- Fermer la session spark en utilisant la méthode stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Affichage des Coefficients du modèle linéaire\n",
    "pprint(linearModel.coefficients)\n",
    "\n",
    "# Fermeture de la session Spark \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aller plus loin - Autres algorithmes de régression\n",
    "\n",
    "Maintenant que vous avez appris à programmer une régression linéaire en utilisant Spark ML, vous n'êtes qu'à quelques pas de maîtriser tout algorithme de régression distribué sous Spark. Pour vous aider à retenir l'essentiel, en voici un aperçu :\n",
    "- 1. Transformer la base de données en format svmlib :\n",
    "      - Sélectionner les variables numériques à utiliser pour la régression.\n",
    "      - Placer la variable à expliquer en première position.\n",
    "      - Mapper un couple (label, vecteur de features) dans un RDD.\n",
    "      - Convertir ce RDD en DataFrame et nommer les variables 'label' et 'features'.\n",
    "- 2. Séparer la base de données en deux échantillons train et test.\n",
    "- 3. Appliquer un modèle de classification.\n",
    "- 4. Evaluer le modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Spark est en constante amélioration et possède aujourd'hui quelques régresseurs notables. Ils sont utilisables de la même façon en important ces fonction depuis pyspark.ml.regression. Vous êtes invité à consulter la documentation pour observer les différents paramètres à prendre en compte pour optimiser ces algorithmes :\n",
    "- LinearRegression() pour effectuer une régression linéaire lorsque le label est présupposé suivre une loi normale.\n",
    "- GeneralizedLinearRegression() pour effectuer une régression linéaire généralisée lorsque le label est présupposé suivre une autre loi que l'on spécifie dans le paramètre family (gaussian, binomial, poisson, gamma).\n",
    "- AFTSurvivalRegression() pour effectuer une analyse de survie.\n",
    "\n",
    "Il est également possible d'utiliser les algorithmes, qui gèrent également les variables catégorielles, détaillés dans l'exercice suivant :\n",
    "\n",
    "- DecisionTreeRegressor() pour un arbre de décision.\n",
    "- RandomForestRegressor() pour une forêt aléatoire d'arbres de décision.\n",
    "- GBTRegressor() pour une forêt d'arbres gradient-boosted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Utilisation des ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de SparkSession et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext en local\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une Session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipelines Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.1. Les variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du fichier 'HR_comma_sep.csv'\n",
    "hr = spark.read.csv('HR_comma_sep.csv', header = True)\n",
    "\n",
    "# Affichage d'un extrait du DataFrame\n",
    "hr.sample(False, 0.001, seed=222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    La variable à prédire est la variable left. Elle indique si l'employé a quitté la boîte volontairement ou non.\n",
    "\n",
    "- (c) Réordonner les variables de façon à placer la variable left en première colonne.\n",
    "- (d) Afficher un résumé des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordonner les variables pour avoir le label en première colonne\n",
    "hr = hr.select( 'left',\n",
    "               'satisfaction_level',\n",
    "               'last_evaluation',\n",
    "               'number_project',\n",
    "               'average_montly_hours',\n",
    "               'time_spend_company',\n",
    "               'Work_accident',\n",
    "               'promotion_last_5years',\n",
    "               'sales',\n",
    "               'salary')\n",
    "\n",
    "# Affichage d'une description des variables\n",
    "hr.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION !!!!\n",
    "\n",
    "\n",
    "La transformation directe de la base de données en svmlib, comme vu dans l'exercice précédent, génère l'erreur suivante :  ValueError: could not convert string to float: 'sales'\n",
    "   \n",
    "   \n",
    "En effet, la fonction DenseVector ne gère pas les strings, il faut donc indexer les variables non numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction StringIndexer du package pyspark.ml.feature permet d'indexer les variables selon la fréquence de leurs modalités: la modalité la plus fréquente aura pour indice 0.0, la modalité suivante 1.0, etc. \n",
    "\n",
    "Pour cela, la fonction prend en entrée une variable et en créé une variable indexée.\n",
    "\n",
    "    La fonction StringIndexer est comme un estimateur, de la même façon qu'une régression ou un arbre de décision. Elle s'utilise donc en deux étapes :\n",
    "\n",
    "    - Créer un indexeur en spécifiant les colonnes d'entrée et de sortie (paramètres inputCol et outputCol) et chercher des modalités dans la base de données grâce à la méthode fit.\n",
    "    - Appliquer l'indexeur à la base de données par le biais de la méthode transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (e) Importer la fonction StringIndexer depuis le package pyspark.ml.feature.\n",
    "- (f) Créer un indexeur SalesIndexer transformant une variable sales en une variable indexedSales.\n",
    "- (g) Indexer la variable sales de hr dans un nouveau DataFrame nommé hrSalesIndexed.\n",
    "- (h) Afficher un extrait de hrSalesIndexed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    La fonction StringIndexer du package pyspark.ml.feature permet d'indexer les variables selon la fréquence de leurs modalités: \n",
    "la modalité la plus fréquente aura pour indice 0.0, la modalité suivante 1.0, etc. Pour cela, la fonction prend en entrée une variable et en créé une variable indexée.\n",
    "\n",
    "La fonction StringIndexer est comme un estimateur, de la même façon qu'une régression ou un arbre de décision. Elle s'utilise donc en deux étapes :\n",
    "\n",
    "- Créer un indexeur en spécifiant les colonnes d'entrée et de sortie (paramètres inputCol et outputCol) et chercher des modalités dans la base de données grâce à la méthode fit.\n",
    "- Appliquer l'indexeur à la base de données par le biais de la méthode transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de StringIndexer du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Création d'un indexeur transformant une variable sales en indexedSales\n",
    "salesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales').fit(hr)\n",
    "\n",
    "# Création d'un DataFrame hrSalesIndexed indexant la variable sales\n",
    "hrSalesIndexed = salesIndexer.transform(hr)\n",
    "\n",
    "# Affichage d'un extrait du DataFrame hrSalesIndexed \n",
    "hrSalesIndexed.sample(False, 0.001, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (i) Importer IndexToString depuis le package pyspark.ml.feature.\n",
    "- (j) Créer une variable salesReconstructed à partir de indexedSales.\n",
    "- (k) Appliquer ce transformateur à hrSalesIndexed en créant une nouvelle table hrSalesReconstructed.\n",
    "- (l) Afficher un extrait de cette table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insérez votre code ici\n",
    "# Import de IndexToString du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# Création d'une nouvelle colonne salesReconstructed\n",
    "SalesReconstructor = IndexToString(inputCol='indexedSales',\n",
    "                                   outputCol='salesReconstructed',\n",
    "                                   labels = salesIndexer.labels)\n",
    "\n",
    "# Appliquer le transformateur SalesReconstructor\n",
    "hrSalesReconstructed = SalesReconstructor.transform(hrSalesIndexed)\n",
    "\n",
    "# Affichage d'un extrait de la base de données\n",
    "hrSalesReconstructed.sample(False, 0.001 , seed = 222).toPandas()\n",
    "\n",
    "### On voit apparaître une nouvelle colonne 'salesReconstructed' égale à la colonne 'sales'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2. Les pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) Importer la fonction Pipeline depuis le package pyspark.ml.\n",
    "- (b) Créer un indexeur SalesIndexer transformant une variable sales en indexedSales.\n",
    "- (c) Créer un indexeur SalaryIndexer transformant une variable salary en indexedSalary.\n",
    "- (d) Créer une Pipeline indexer qui applique les deux transformations.\n",
    "- (e) Indexer les variables de hr dans un nouveau DataFrame nommé hrIndexed.\n",
    "- (f) Afficher un extrait de hrIndexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de Pipeline du package pyspark.ml\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Création des indexeurs\n",
    "SalesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales')\n",
    "SalaryIndexer = StringIndexer(inputCol='salary', outputCol='indexedSalary')\n",
    "\n",
    "# Création d'une Pipeline\n",
    "indexer = Pipeline(stages =  [SalaryIndexer, SalesIndexer])\n",
    "\n",
    "# Indexer les variables de hr\n",
    "hrIndexed = indexer.fit(hr).transform(hr)\n",
    "\n",
    "# Affichage d'un extrait de hrIndexed\n",
    "hrIndexed.sample(False, 0.001 , seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.3. Mise en forme de la base en format svmlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) Créer une base de données hrNumeric excluant les anciennes variables non indexées.\n",
    "- (b) Créer hrLibsvm, une base de données au format svmlib à partir de hrNumeric.\n",
    "- (c) Afficher un extrait de la nouvelle base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de DenseVector du package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Création d'une base de données excluant les variables non indexées\n",
    "hrNumeric = hrIndexed.select('left',\n",
    "                             'satisfaction_level',\n",
    "                             'last_evaluation',\n",
    "                             'number_project',\n",
    "                             'average_montly_hours',\n",
    "                             'time_spend_company',\n",
    "                             'Work_accident',\n",
    "                             'promotion_last_5years',\n",
    "                             'indexedSales',\n",
    "                             'indexedSalary')\n",
    "\n",
    "# Création d'une variable DenseVector contenant les features en passant par la structure RDD\n",
    "hrRdd = hrNumeric.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Transformation en DataFrame et nommage des variables pour obtenir une base au format svmlib\n",
    "hrLibsvm = spark.createDataFrame(hrRdd, ['label', 'features'])\n",
    "\n",
    "# Affichage d'un extrait de hrLibsvm\n",
    "hrLibsvm.sample(False, .001, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.4. Application d'un classifieur Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données d'apprentissage actuelle est au format svmlib. Il ne reste qu'à spécifier que:\n",
    "\n",
    "- La variable label est catégorielle.\n",
    "- Certaines features sont catégorielles et d'autres sont continues.\n",
    "\n",
    "\n",
    "Pour cela, deux transformateurs seront créés et intégrés dans une Pipeline générale :\n",
    "\n",
    "\n",
    "    -# Pour pouvoir classifier sur notre label,\n",
    "    -# on crée une variable 'indexedLabel' comme vu précédemment.\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(hr_ml)\n",
    "\n",
    "    -# Pour les features, on utilise la fonction 'VectorIndexer'\n",
    "    -# qui définie un seuil de nombre de modalités:\n",
    "    -# ici, si une variable a plus de 5 modalités, alors elle sera considérée comme continue\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\",\n",
    "                               outputCol=\"indexedFeatures\",\n",
    "                               maxCategories = 5).fit(hr_ml)\n",
    "                               \n",
    "                               \n",
    "La fonction VectorIndexer a été développée de façon à n'indexer que les variables qui ont moins d'un certain nombre de modalités. Ce nombre est spécifié par l'argument maxCategories. Il faut donc déterminer ce paramètre en regardant le nombre maximal de modalités des variables catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de VectorIndexer du package pyspark.ml.feature\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Création d'un transformateur indexant les features\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\",\n",
    "                               outputCol=\"indexedFeatures\",\n",
    "                               maxCategories = 10).fit(hrLibsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du classifieur RandomForestClassifier du package pyspark.ml.classification\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Création des transformateurs\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(hrLibsvm)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories = 10).fit(hrLibsvm)\n",
    "\n",
    "# Création d'un classifieur \n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", predictionCol='prediction', seed = 222)\n",
    "\n",
    "# Création d'un transformateur permettant de rétablir les labels des prédictions\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Création d'une Pipeline \n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Décomposition des données en deux ensembles: données d'entraînement et de test\n",
    "(train, test) = hrLibsvm.randomSplit([0.7, 0.3], seed = 222)\n",
    "\n",
    "# Apprentissage du modèle en utilisant les données d'entraînement\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du classifieur RandomForestClassifier du package pyspark.ml.classification\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Création des transformateurs\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(hrLibsvm)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories = 10).fit(hrLibsvm)\n",
    "\n",
    "# Création d'un classifieur \n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", predictionCol='prediction', seed = 222)\n",
    "\n",
    "# Création d'un transformateur permettant de rétablir les labels des prédictions\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Création d'une Pipeline \n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Décomposition des données en deux ensembles: données d'entraînement et de test\n",
    "(train, test) = hrLibsvm.randomSplit([0.7, 0.3], seed = 222)\n",
    "\n",
    "# Apprentissage du modèle en utilisant les données d'entraînement\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.5. Evaluation du modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le modèle d'apprentissage automatique construit, il est important de vérifier la fiabilité des prédictions, à la fois pour le comparer à d'autres modèles de classification mais également pour optimiser ses paramètres.\n",
    "\n",
    "Pour cela, il existe un sous-module pyspark.ml.evaluation contenant toutes les métriques d'évaluation. En particulier, vous y trouverez la fonction MulticlassClassificationEvaluator permettant d'évaluer des modèles de classification.\n",
    "\n",
    "Cette fonction prend 3 arguments principaux :\n",
    "\n",
    "metricName : métrique à utiliser, typiquement : 'accuracy'.\n",
    "labelCol : nom de la colonne à prédire.\n",
    "predictionCol : nom de la colonne de prédictions.\n",
    "L'évaluateur créé possède une méthode evaluate permettant de l'appliquer à un échantillon.\n",
    "\n",
    "- (a) Importer la fonction MulticlassClassificationEvaluator.\n",
    "- (b) Créer evaluator l'évaluateur d'accuracy du modèle.\n",
    "- (c) Calculer et afficher la précision accuracy de la prédiction sur l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import d'un évaluateur MulticlassClassificationEvaluator du package pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Création d'un évaluateur \n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "                                              labelCol= 'indexedLabel',\n",
    "                                              predictionCol= 'prediction')\n",
    "# Calcul et affichage de la précision du modèle \n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de SparkSession et de SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Création d'un SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Création d'une session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML Tuning\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.1. Importation de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la base de données brute\n",
    "df_full = spark.read.csv('YearPredictionMSD.txt', header=False)\n",
    "\n",
    "# On infère les bons types des colonnes\n",
    "from pyspark.sql.functions import col\n",
    "exprs = [col(c).cast(\"double\") for c in df_full.columns[1:13]]\n",
    "\n",
    "df_casted = df_full.select(df_full._c0.cast('int'),\n",
    "                           *exprs)\n",
    "\n",
    "# Enfin, par soucis de rapidité des calculs,\n",
    "# on ne traitera qu'un extrait de la base de données \n",
    "df = df_casted.sample(False, .1, seed = 222)\n",
    "\n",
    "df.sample(False, .001, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (b) Convertir cette base de données au format svmlib dans une variable df_ml.\n",
    "- (c) Afficher un extrait de cette base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Conversion de la base de données au format svmlib\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "df_ml.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (d) Séparer df_ml en un ensemble train et un ensemble test comprenant respectivement 80% et 20% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Décomposition des données en deux ensembles d'entraînement et de test\n",
    "# Par défaut l'échantillon est aléatoirement réparti\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous allons instancier un modèle de régression linéaire.\n",
    "\n",
    "- (e) Importer la fonction LinearRegression depuis le package pyspark.ml.regression.\n",
    "- (f) Créer un estimateur lr permettant d'effectuer une régression linéaire entre le label 'label'et les features 'features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de LinearRegression du package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Création d'un estimateur LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2. Création d'une grille de paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de ParamGridBuilder du package pyspark.ml.tuning\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Création d'une grille de paramètres\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(lr.regParam, [0, 0.5, 1]).\\\n",
    "    addGrid(lr.elasticNetParam, [0, 0.5, 1]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.3. Choix d'une métrique d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de RegressionEvaluator du package pyspark.ml.evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Création d'un évaluateur ayant pour métrique d'évaluation r2\n",
    "ev = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='label',\n",
    "                                metricName='r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.4. Réglage des paramètres par validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de CrossValidator du package pyspark.ml.tuning\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Création d'un objet CrossValidator à 3 folds\n",
    "cv = CrossValidator(estimator = lr,\n",
    "                    estimatorParamMaps = param_grid,\n",
    "                    evaluator=ev,\n",
    "                    numFolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.5. Application du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la bibliothèque time et calcul du temps au début de l'exécution (t0)\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "ev = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='r2')\n",
    "cv = CrossValidator(estimator = lr, estimatorParamMaps = param_grid, evaluator = ev, numFolds = 3)\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "tt = time() - t0\n",
    "print(\"Réalisé en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des prédictions des données d'entraînement\n",
    "pred_train = cv_model.transform(train)\n",
    "\n",
    "# Calcul des prédictions des données de test\n",
    "pred_test  = cv_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.setMetricName('rmse').evaluate(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.6. Exploitation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des coefficients du modèle\n",
    "cv_model.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les paramètres de la grille obtenus pour le meilleur modèle ne sont pas accessibles directement, mais sont stockés au sein de l'objet java :\n",
    "\n",
    "    cv_model.bestModel._java_obj.getRegParam()\n",
    "    cv_model.bestModel._java_obj.getElasticNetParam()\n",
    "\n",
    "Cette information est utile pour vérifier que notre grille est bien adaptée au modèle : il faut éviter que le paramètre choisi soit sur un bord de notre intervalle.\n",
    " \n",
    " \n",
    " Il faut ici passer par l'objet java parce que cette option n'est pas encore disponible directement dans PySpark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
